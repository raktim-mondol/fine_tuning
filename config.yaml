# Model configuration
model:
  model_id: "google/medgemma-4b-it"  # Updated to instruction-tuned version
  torch_dtype: "bfloat16"  # Options: bfloat16, float16, float32
  attn_implementation: "eager"  # Options: eager, flash_attention_2
  device_map: "auto"
  trust_remote_code: true

# Quantization configuration (4-bit QLoRA)
quantization:
  load_in_4bit: true
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_storage: "bfloat16"

# LoRA configuration
lora:
  r: 16                    # Rank of adaptation matrices (8, 16, 32, 64)
  lora_alpha: 16           # Scaling parameter (usually same as r)
  lora_dropout: 0.05       # Dropout rate for LoRA layers
  target_modules: "all-linear"  # Apply to all linear layers
  bias: "none"             # Don't adapt bias terms
  task_type: "CAUSAL_LM"
  modules_to_save:         # Additional modules to save
    - "lm_head"
    - "embed_tokens"

# Training configuration (using SFTConfig)
training:
  output_dir: "medgemma-4b-it-sft-lora-histopath"
  num_train_epochs: 1                        # Number of training epochs
  per_device_train_batch_size: 4             # Training batch size per GPU
  per_device_eval_batch_size: 4              # Evaluation batch size per GPU
  gradient_accumulation_steps: 4             # Accumulate gradients over steps
  gradient_checkpointing: true               # Enable gradient checkpointing
  optim: "adamw_torch_fused"                 # Use fused AdamW optimizer
  logging_steps: 50                          # Log every N steps
  save_strategy: "epoch"                     # When to save checkpoints
  eval_strategy: "steps"                     # When to evaluate
  eval_steps: 50                             # Number of steps between evaluations
  learning_rate: 2e-4                        # Learning rate based on QLoRA paper
  bf16: true                                 # Use bfloat16 precision
  max_grad_norm: 0.3                         # Max gradient norm based on QLoRA paper
  warmup_ratio: 0.03                         # Warmup ratio based on QLoRA paper
  lr_scheduler_type: "linear"                # Use linear learning rate scheduler
  push_to_hub: true                          # Push model to Hub
  report_to: "tensorboard"                   # Report metrics to tensorboard
  gradient_checkpointing_kwargs:
    use_reentrant: false                     # Set gradient checkpointing to non-reentrant
  dataset_kwargs:
    skip_prepare_dataset: true               # Skip default dataset preparation
  remove_unused_columns: false               # Columns needed for data collator
  label_names:
    - "labels"                               # Input keys that correspond to labels

# Data configuration
data:
  data_path: ""                              # TO BE SET: Path to your histopath dataset
  train_split: 0.7                           # Training data fraction
  val_split: 0.15                            # Validation data fraction  
  test_split: 0.15                           # Test data fraction
  max_patches_per_patient: 10                # Maximum patches per patient
  min_patches_per_patient: 1                 # Minimum patches required per patient
  train_size: 9000                           # Number of training samples
  validation_size: 1000                      # Number of validation samples
  image_extensions:                          # Supported image formats
    - ".jpg"
    - ".jpeg"
    - ".png"
    - ".tiff"
    - ".tif"
    - ".bmp"

# General configuration
seed: 42                                     # Random seed for reproducibility
hf_token: ""                                # TO BE SET: Your HuggingFace token
